{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ed5825",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3cb23a",
   "metadata": {},
   "source": [
    "### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae8e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/28 10:04:03 WARN Utils: Your hostname, Soumyas-MacAir.local resolves to a loopback address: 127.0.0.1; using 10.0.0.43 instead (on interface en0)\n",
      "22/03/28 10:04:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/soumya/Library/Python/3.8/lib/python/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/28 10:04:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, StructField, StructType, DataTypeSingleton\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"FlightDataAggregation\")\n",
    "        .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aea9fd",
   "metadata": {},
   "source": [
    "### Wrapping Spark options in named tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b764084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "allOptions = namedtuple(\"allOptions\", \"csvFormat header inferSchema\")\n",
    "sparkOptions = allOptions(\"csv\", \"header\", \"inferSchema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36810108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- origin_code: string (nullable = true)\n",
      " |-- origin_airport: string (nullable = true)\n",
      " |-- origin_city: string (nullable = true)\n",
      " |-- origin_state: string (nullable = true)\n",
      " |-- dest_code: string (nullable = true)\n",
      " |-- dest_airport: string (nullable = true)\n",
      " |-- dest_city: string (nullable = true)\n",
      " |-- dest_state: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData =\"/Users/soumya/Documents/POC/Spark-3-data/section_4/flight-summary.csv\"\n",
    "flight_summary_df = (spark.read.format(sparkOptions.csvFormat)\n",
    "                     .option(sparkOptions.header,\"true\")\n",
    "                     .option(sparkOptions.inferSchema, \"true\")\n",
    "                     .load(flightData))\n",
    "flight_summary_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01b510",
   "metadata": {},
   "source": [
    "### String definition of schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b823758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstName: string (nullable = false)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastName: string (nullable = false)\n",
      " |-- zipCode: string (nullable = false)\n",
      " |-- sex: string (nullable = false)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-------+---+------+\n",
      "|firstName|middlename|lastName|zipCode|sex|salary|\n",
      "+---------+----------+--------+-------+---+------+\n",
      "|   Soumya|          |    Kole|  36636|  M|    -1|\n",
      "|      Foo|       Bar|     xxx|       |   |  9000|\n",
      "+---------+----------+--------+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = \"firstName:String:false,middlename:String:true,lastName:String:false,zipCode:String:false,sex:String:false,salary:Int:true\"\n",
    "simpleData = (\n",
    "  Row(\"Soumya\",\"\",\"Kole\",\"36636\",\"M\",-1),\n",
    "  Row(\"Foo\",\"Bar\",\"xxx\",\"\",\"\",9000)\n",
    ")\n",
    "\n",
    "def convert_to_bool(bool_string: str) -> bool:\n",
    "    if bool_string.upper().strip() == \"TRUE\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def match_type(in_type: str) -> DataTypeSingleton:\n",
    "    type_dict = {\n",
    "        \"INT\" : IntegerType(),\n",
    "        \"DOUBLE\": DoubleType(),\n",
    "        \"STRING\": StringType()\n",
    "    }\n",
    "    return type_dict.get(in_type.upper(), StringType())\n",
    "\n",
    "\n",
    "def infer_type(field: str) -> StructField:\n",
    "    splits = field.split(\":\")\n",
    "    col_name, data_type, nullable = splits[0], splits[1], convert_to_bool(splits[2]) \n",
    "    return StructField(col_name, match_type(data_type), nullable)\n",
    "tuple_of_struct_fields = *(infer_type(col) for col in cols.split(\",\")),\n",
    "schema = StructType(tuple_of_struct_fields)\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(simpleData)\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90586c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79639ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
